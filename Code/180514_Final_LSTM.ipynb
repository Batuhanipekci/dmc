{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gerome\\Desktop\\DMC 2018\\Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% cd \"C:\\Users\\Gerome\\Desktop\\DMC 2018\\Predictions\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytrends.request import TrendReq\n",
    "from functools import reduce\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "py.init_notebook_mode(connected=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\", sep='|')\n",
    "train_data['key'] = train_data['pid'].astype(str) + train_data['size'].astype(str)\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "table = pd.pivot_table(train_data, values='units', index=['key'], aggfunc=np.sum)\n",
    "table.sort_values('units', ascending=False, inplace=True)\n",
    "relevant_pids = table.loc[table['units'] >= 100]\n",
    "relevant_pids.reset_index(drop=False, inplace=True)\n",
    "relevant_pids_list = list(relevant_pids['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_data = pd.read_csv(\"items.csv\", sep='|')\n",
    "item_data['key'] = item_data['pid'].astype(str) + item_data['size'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyword = 'Nike'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "Nike_df = pytrend.interest_over_time()\n",
    "del Nike_df[\"isPartial\"]\n",
    "Nike_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "keyword = 'adidas'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "adidas_df = pytrend.interest_over_time()\n",
    "del adidas_df[\"isPartial\"]\n",
    "adidas_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "keyword = 'PUMA'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "PUMA_df = pytrend.interest_over_time()\n",
    "del PUMA_df[\"isPartial\"]\n",
    "PUMA_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "keyword = 'Jako'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "Jako_df = pytrend.interest_over_time()\n",
    "del Jako_df[\"isPartial\"]\n",
    "Jako_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "keyword = 'Jordan'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "Jordan_df = pytrend.interest_over_time()\n",
    "del Jordan_df[\"isPartial\"]\n",
    "Jordan_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "keyword = 'Sport2000'\n",
    "pytrend = TrendReq(hl='de-DE')\n",
    "pytrend.build_payload(kw_list=[keyword], timeframe='2017-10-01 2018-02-28', geo='DE')\n",
    "Sport2000_df = pytrend.interest_over_time()\n",
    "del Sport2000_df[\"isPartial\"]\n",
    "Sport2000_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "dfs = [Nike_df, adidas_df, PUMA_df, Jako_df, Jordan_df, Sport2000_df]\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='date'), dfs)\n",
    "df_final.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelling item 12985L (item no. 1) in progress...\n",
      "(123, 1, 10) (123,) (123, 1, 10) (123,)\n",
      "Sell off date for product 12985L is 2018-02-05\n",
      "Item 12985L completed. 50.0 % completed.\n",
      "Iteration took 112.87145590782166 seconds\n",
      "Modelling item 1041643 1/3 (item no. 2) in progress...\n",
      "(86, 1, 10) (86,) (86, 1, 10) (86,)\n",
      "Sell off date for product 1041643 1/3 is 2018-02-06\n",
      "Item 1041643 1/3 completed. 100.0 % completed.\n",
      "Iteration took 67.60686683654785 seconds\n",
      "Total time was180.48432302474976 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_list = []\n",
    "\n",
    "for idx, element in enumerate(relevant_pids_list):\n",
    "    \n",
    "    start_time_it = time.time()\n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(1)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('Modelling item ' + element + ' (item no. ' + str(idx+1) + ')' + ' in progress...')\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    data_dict['key'] = element\n",
    "    \n",
    "    data_dict['element_in_pids'] = idx\n",
    "    \n",
    "    sub_item = item_data.loc[item_data['key'] == element]\n",
    "    \n",
    "    sub_train = train_data.loc[train_data['key'] == element]\n",
    "    sub_train = sub_train.sort_values('date', ascending=True)\n",
    "    sub_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    sub_brand = sub_item.iloc[0]['brand']\n",
    "    data_dict['brand'] = sub_brand\n",
    "    sub_stock = int(sub_item.iloc[0]['stock'])\n",
    "    data_dict['stock'] = sub_stock\n",
    "    \n",
    "    date_index = pd.date_range(start='2017-10-01', end='2018-02-28', freq='D')\n",
    "    date_index = pd.DataFrame(date_index)\n",
    "    date_index.columns = ['date']\n",
    "    \n",
    "    date_sub_train_merged = pd.merge(date_index, sub_train, on='date', how='left')\n",
    "    \n",
    "    date_sub_train_merged.set_index('date', inplace=True)\n",
    "    del date_sub_train_merged['pid']\n",
    "    del date_sub_train_merged['size']\n",
    "    \n",
    "    date_sub_train_merged = date_sub_train_merged.merge(pd.DataFrame(df_final[sub_brand]),left_index=True, right_index=True)\n",
    "    \n",
    "    date_sub_train_merged.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    date_sub_train_merged['weekday_name'] = date_sub_train_merged['date'].dt.weekday_name\n",
    "    \n",
    "    date_sub_train_merged['is_weekend'] = np.where((date_sub_train_merged['weekday_name'] == 'Saturday') | \n",
    "                                               (date_sub_train_merged['weekday_name'] == 'Sunday'), \n",
    "                                               1, 0)\n",
    "    \n",
    "    model_df = date_sub_train_merged[['units',sub_brand,'weekday_name','is_weekend']]\n",
    "    \n",
    "    average_sales = model_df['units'].mean(skipna=True)\n",
    "    data_dict['average_sales_per_day'] = average_sales\n",
    "    \n",
    "    model_df = model_df.merge(pd.DataFrame(date_sub_train_merged['date']),left_index=True, right_index=True)\n",
    "    \n",
    "    train_set = model_df[:123]\n",
    "    predict_set = model_df[123:]\n",
    "\n",
    "    train_set.dropna(inplace=True)\n",
    "\n",
    "    model_df = pd.concat([train_set, predict_set])\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    model_df.loc[:,[sub_brand]] = scaler.fit_transform(np.array(model_df.loc[:,[sub_brand]]).reshape(-1,1))\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    null_index = model_df['units'].isnull()\n",
    "    model_df.loc[~null_index, ['units']] = scaler.fit_transform(model_df.loc[~null_index, ['units']])\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    model_df['weekday_name'] = encoder.fit_transform(model_df['weekday_name'])\n",
    "    model_df['is_weekend'] = encoder.fit_transform(model_df['is_weekend'])\n",
    "\n",
    "    hotencoder = preprocessing.OneHotEncoder()\n",
    "    weekday = np.array(model_df['weekday_name']).reshape(-1,1)\n",
    "    weekend = np.array(model_df['is_weekend']).reshape(-1,1)\n",
    "    hotencoder.fit(weekday)\n",
    "    onehot_weekday = hotencoder.transform(weekday).toarray()\n",
    "    hotencoder.fit(weekend)\n",
    "    onehot_weekend = hotencoder.transform(weekend).toarray()\n",
    "    weekday_df = pd.DataFrame(onehot_weekday)\n",
    "    weekend_df = pd.DataFrame(onehot_weekend)\n",
    "    \n",
    "    model_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merge1 = pd.merge(model_df, weekday_df, how='left', left_index=True, right_index=True)\n",
    "    model_df = pd.merge(merge1, weekend_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    del model_df['weekday_name']\n",
    "    del model_df['is_weekend']\n",
    "    \n",
    "    model_df.set_index('date', inplace=True)\n",
    "    \n",
    "    model_df.columns = ['units', sub_brand, 'weekday_0', 'weekday_1','weekday_2',\n",
    "                   'weekday_3','weekday_4','weekday_5', 'weekday_6',\n",
    "                   'is_weekend_0', 'is_weekend_1']\n",
    "    \n",
    "    train_set = model_df[:-28]\n",
    "    predict_set = model_df[-28:]\n",
    "\n",
    "    values_predict = predict_set.values\n",
    "    values_predict = values_predict.astype('float32')\n",
    "    predict_X = values_predict[:, 1:]\n",
    "    predict_X = predict_X.reshape((predict_X.shape[0], 1, predict_X.shape[1]))\n",
    "\n",
    "    values_train = train_set.values\n",
    "    values_train = values_train.astype('float32')\n",
    "    \n",
    "    train_X, train_y = values_train[:, 1:], values_train[:, 0]\n",
    "    test_X, test_y = values_train[:, 1:], values_train[:, 0]\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "    multi_model = Sequential()\n",
    "    multi_model.add(LSTM(500, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    multi_model.add(Dense(1))\n",
    "    multi_model.compile(loss='mse', optimizer='adam')\n",
    "    multi_history = multi_model.fit(train_X, train_y, epochs=100, batch_size=30, validation_data=(train_X, train_y), verbose=0, shuffle=False)\n",
    "    \n",
    "    yhat = multi_model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    inv_yhat = concatenate((yhat, test_X[:, :1]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    \n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, :1]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    \n",
    "    data_dict['rmse_full_set'] = rmse\n",
    "    \n",
    "    y_predict = multi_model.predict(predict_X)\n",
    "    predict_X = predict_X.reshape((predict_X.shape[0], predict_X.shape[2]))\n",
    "    inv_y_predict = scaler.inverse_transform(y_predict)\n",
    "    inv_y_predict = inv_y_predict[:,0]\n",
    "    \n",
    "    import datetime as dt\n",
    "\n",
    "    start_day = \"2018-02-01\"\n",
    "\n",
    "    start_day = dt.datetime.strptime(start_day, '%Y-%m-%d').date()\n",
    "\n",
    "    sell_off_date = start_day + dt.timedelta(days=int(np.argmin(np.cumsum(inv_y_predict) <= sub_stock)))\n",
    "    \n",
    "    data_dict['sell_off_date'] = sell_off_date\n",
    "\n",
    "    print ('Sell off date for product ' + element + ' is ' + str(sell_off_date))\n",
    "    \n",
    "    inv_y_predict_df = pd.DataFrame(inv_y_predict)\n",
    "    inv_y_predict_df.columns = ['prediction']\n",
    "    inv_y_predict_df.index = predict_set.index\n",
    "    \n",
    "    import string\n",
    "    \n",
    "    invalidChars = set(string.punctuation.replace(\"_\", \"\"))\n",
    "    \n",
    "    if any(char in invalidChars for char in element):\n",
    "        \n",
    "        inv_y_predict_df.to_csv(str(idx) + '_prediction.csv')\n",
    "          \n",
    "    else:\n",
    "        \n",
    "        inv_y_predict_df.to_csv(element + '_prediction.csv')\n",
    "    \n",
    "    all_list.append(data_dict)\n",
    "    \n",
    "    output_df = pd.DataFrame(all_list)\n",
    "    output_df.to_csv('predictions_output.csv')\n",
    "    \n",
    "    print ('Item ' + element + ' completed. ' + str((idx+1)/len(relevant_pids_list)*100) + ' % completed.')\n",
    "    \n",
    "    print('Iteration took ' + \"%s seconds\" % (time.time() - start_time_it))\n",
    "\n",
    "print('Total time was ' + \"%s seconds\" % (time.time() - start_time_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
